<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #5858FA;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 40px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/csail_logo_32.png">
  <title>Cheng-I Lai</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
             <p align="center">
                <name>Cheng-I Jeff Lai</name>
              </p>
              <p align="center" valign="top">  
                clai24 at mit dot edu
              </p>
              <p>
                I will begin my Ph.D. in computer science at <a href="https://www.csail.mit.edu">MIT Computer Science and Artificial Intelligence Laboratory</a> and be advised by <a href="http://people.csail.mit.edu/jrg/">James Glass</a>, starting in Fall 2019. I received my B.S. in electrical engineering from <a href="https://www.jhu.edu">Johns Hopkins University</a>, where I was advised by <a href="https://engineering.jhu.edu/ece/faculty/najim-dehak/">Najim Dehak</a> and <a href="https://www.clsp.jhu.edu/faculty/jesus-villalba/">Jesús Villalba</a>. In the past, I have spent time at <a href="https://www.clsp.jhu.edu">CLSP</a>, <a href="http://www.cstr.ed.ac.uk">CSTR</a>, and <a href="https://hltcoe.jhu.edu">HLTCoE</a>. 
              </p>
              <p>
                I study spoken and natural language processing, specifically in the context of speaker recognition, machine translation, and speech synthesis.
              </p>
              <p align=center>
                <a href="data/jeff_lai_cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=mV4mRm0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/jefflai108">GitHub</a> &nbsp/&nbsp
                <a href="https://medium.com/@jefflai108">Medium</a> &nbsp/&nbsp
                <a href="https://mrjlai.weebly.com/videos.html">Videos</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/jefflai108/">LinkedIn </a>
              </p>
            </td>
            <td width="33%">
              <img src="images/jeff_profile_pic_square_280.png">
            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>News</heading>
              <p>
              <ul>
                <li>In summer 2019, I will be visiting <a href="https://www.nii.ac.jp/en/">National Institute of Informatics</a> in Tokyo, Japan.</li>
                <li>In March 2019, our system was ranked 3rd in the <a href="http://www.asvspoof.org">ASVspoof 2019 Challenge</a>.</li>
                <li>In Febuary 2019, I received Merrill Lynch Fellowship for my graduate study at MIT.</li>
                <li>In December 2018, I received my bachelor from Johns Hopkins!</li>
                <li>In summer 2018, I visited University of Edinburgh, supported by Vredenburg Scholarship.</li>
              </ul>  
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
            <td width="18.75%"><img src="images/lai2019assert.png" align="middle" alt="lai2019assert" width="150" height="287" style="border-style: none">
              <td width="81.25%" valign="middle">
                <p>
                  <a href="data/lai2019assert.pdf">
                    <papertitle>ASSERT: Anti-Spoofing with Squeeze-Excitation and Residual neTworks</papertitle>
                  </a>
                  <br>
                    <strong>Cheng-I Lai</strong>,
                    <a href="https://scholar.google.com.hk/citations?user=4iFb9qYAAAAJ&hl=en/">Nanxin Chen</a>,
                    <a href="https://www.clsp.jhu.edu/faculty/jesus-villalba/">Jesús Villalba</a>,
                    <a href="https://engineering.jhu.edu/ece/faculty/najim-dehak/">Najim Dehak</a>
                  <br>
                  Submitted to <em>Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</em>, 2019
                  <br>
                    <a href="https://arxiv.org/abs/1904.01120">arxiv</a> /
                    <a href="https://github.com/jefflai108/ASSERT">code</a> /
			        <a href="data/lai2019assert.bib">bibtex</a>
                </p>
                <p>
                    ASSERT is a pipeline for DNN-based approach to anti-spoofing, composed of four components: feature engineering, DNN models, network optimization and system combination, where the DNN models are variants of squeeze-excitation and residual networks.
                </p>
                <p>
                  Our systems are evaluated on the ASVspoof 2019 dataset.
                </p>
              </td>
          </tr>

          <tr>
            <td width="18.75%"><img src="images/lai2019contrastive.png" align="middle" alt="lai2019contrastive" width="150" height="100" style="border-style: none">
              <td width="81.25%" valign="middle">
                <p>
                  <a href="data/lai2019contrastive.pdf">
                    <papertitle>Contrastive Predictive Coding Based Feature for Automatic Speaker Verification</papertitle>
                  </a>
                  <br>
                    <strong>Cheng-I Lai</strong>
                  <br>
                  <em>Bachelor Thesis</em>, 2018
                  <br>
                    <a href="https://arxiv.org/abs/1904.01575">arxiv</a> /
                    <a href="https://github.com/jefflai108/Contrastive-Predictive-Coding-PyTorch">code</a> /
			        <a href="data/lai2019contrastive.bib">bibtex</a>
                </p>
                <p>
                    This thesis incorporates Contrastive Predictive Coding (CPC) features into standard automatic speaker verification systems.
                </p>
                <p>
                    Our systems are evaluated on the LibriSpeech dataset.
                </p>
              </td>
          </tr>

          <tr>
            <td width="18.75%"><img src="images/double_decoder.png" align="middle" alt="double_decoder" width="150" height="198" style="border-style: none">
              <td width="81.25%" valign="middle">
                <p>
                  <a href="data/double_decoder.pdf">
                    <papertitle>Controlling the Reading Level of MachineTranslation Output</papertitle>
                  </a>
                  <br>
                  Kelly Marchisio,
                  Jialiang Guo,
                  <strong>Cheng-I Lai</strong>,
                  <a href="http://www.cs.jhu.edu/~phi/">Philipp Koehn</a>
                  <br>
                  <em>MT Summit</em>, 2019
                  <br>
			        <a href="https://docs.google.com/presentation/d/1tq0PF9mV7YCRStVlGo2J1ddGp1ueOhmK5OEqRSLpfd4/edit?usp=sharing">slides</a>
                </p>
                <p>
                    We developed methodologies to controll the complexity and reading level of machine translation systems outputs, and adopted three readability tests with extensive analyses. 
                </p>
                <p>
                    Our systems are evaluated on the newstest2013, OpenSubtitles2018, and Paracrawl datasets.
                </p>
              </td>
          </tr>

          <tr>
            <td width="18.75%"><img src="images/lai2019attentive.png" align="middle" alt="lai2019attentive" width="150" height="223" style="border-style: none">
              <td width="81.25%" valign="middle">
                <p>
                  <a href="data/lai2019attentive.pdf">
                    <papertitle>Attentive Filtering Networks for Audio Replay Attack Detection</papertitle>
                  </a>
                  <br>
                    <strong>Cheng-I Lai</strong>,
                    <a href="https://www.l2f.inesc-id.pt/w/Alberto_Abad_Gareta">Alberto Abad</a>,
                    <a href="http://homepages.inf.ed.ac.uk/korin/sitenew/index.html">Korin Richmond</a>,
                    <a href="https://nii-yamagishilab.github.io/">Junichi Yamaghashi</a>,
                    <a href="https://engineering.jhu.edu/ece/faculty/najim-dehak/">Najim Dehak</a>,
                    <a href="http://homepages.inf.ed.ac.uk/simonk/">Simon King</a>
                  <br>
                  <em>Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019
                  <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/8682640">arxiv</a> /
                    <a href="https://github.com/jefflai108/Attentive-Filtering-Network">code</a> /
			        <a href="data/lai2019attentive.bib">bibtex</a>
                </p>
                    We proposed a new countermeasure for speech spoofing attacks: Attentive Filtering Network, which is composed of an attention-based filtering mechanism that enhances feature representations in both the frequency and time domains, and a ResNet-based classifier.
			    </p>
                <p>
                    Our systems are evaluated on the ASVspoof 2017 2.0 dataset.
                </p>

              </td>
          </tr>
 
          <tr>
            <td width="18.75%"><img src="images/nidadavolu2018investigation_150.png" align="middle" alt="nidadavolu2018investigation" width="150" height="78" style="border-style: none">
              <td width="81.25%" valign="middle">
                <p>
                  <a href="data/nidadavolu2018investigation.pdf">
                    <papertitle>Investigation on Bandwidth Extension for Speaker Recognition</papertitle>
                  </a>
                  <br>
                    Phani Nidadavolu, 
                    <strong>Cheng-I Lai</strong>,
                    <a href="https://www.clsp.jhu.edu/faculty/jesus-villalba/">Jesús Villalba</a>,
                    <a href="https://engineering.jhu.edu/ece/faculty/najim-dehak/">Najim Dehak</a>
                  <br>
                  <em>Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</em>, 2018
                  <br>
                    <a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/2394.pdf">arxiv</a> /
			        <a href="data/nidadavolu2018investigation.bib">bibtex</a>
                </p>
                <p>
                    We addressed sampling mismatches between wideband (WB) and narrowband (NB) speech data with two techniques: (1) a Bandwidth Extension network that predicts WB features given NB features as input, and (2) a low-pass filter interpolator. 
                </p>
                <p>
                    Our systems are evaluated on the Speakers In The Wild (SITW) dataset.
                </p>
              </td>
            </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Invited Talks</heading>
              <p>
              <ul>
                <li>Deep Learning in Artificial Intelligence, May 2019.  
                    <br>
                    Chinese Medicine Research Seminar, China Medical University.
                </li>
                <br>
                <li><a href="https://drive.google.com/file/d/1HTzJBq7RWjxpYeyrQ54Jq5QZoanH-uWo/view?usp=sharing">Deep Learning Frameworks for Anti-Spoofing</a>, October 2018.  
                    <br>
                    Gulf Coast Undergraduate Research Symposium, Rice University.
                </li>
                <br>
                <li><a href="https://drive.google.com/file/d/1tv2tkREV8PClpggKo8RsUl3wPanplc-0/view?usp=sharing">Attentive Filtering Network for Audio Replay Attacks Detection</a>, October 2018.  
                    <br>
                    Center for Language and Speech Processing Seminar, Johns Hopkins University.
                </li>
                <br>
                <li><a href="https://drive.google.com/file/d/1L1cIDNbSjU2WUgRLJT0vY-TUWRMYIqB0/view?usp=sharing">Attentive Filtering Network: An Augmentation of ResNet for Audio Replay Attacks Detection</a>, August 2018.  
                    <br>
                    Centre for Speech Technology Research Seminar, University of Edinburgh.
                </li>
              </ul>  
              </p>
            </td>
          </tr>
        </table>
         
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  <a href="https://jonbarron.info">website template</a>
                  </font>
              </p>
            </td>
          </tr>
        </table>

        </td>
    </tr>
  </table>
</body>

</html>
